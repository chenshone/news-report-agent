Andrej Karpathy 的 “LLM Council” 项目深度解析

LLM Council 概念示意图：多个不同的 AI 模型作为委员会成员协作回答问题，由主席模型综合意见形成最终答案。

LLM Council 是由 Andrej Karpathy 在 GitHub 上开源的一个实验性项目，其核心思想是让多个大型语言模型（LLM）组成一个“委员会”，共同协作来回答用户提出的难题 ￼ ￼。与传统上仅依赖单一模型回答不同，LLM Council 将多个模型的智慧进行集成：每个模型先独立给出解答，然后模型之间相互评审投票，最后由一个“主席”模型裁决整合作出最终答案 ￼。Karpathy 提出这一思路的动机在于解决不同模型回答不一致的问题——以往我们常发现 GPT-4、Claude、Gemini 等模型给出截然不同的答案，而用户不得不在它们之间反复比对 ￼。LLM Council 则通过让多个顶尖模型一起辩论来趋向更准确的答案，发挥“群体智慧”的优势 ￼。下面我们将从协作机制、系统架构、Prompt 设计策略，以及与其他多模型协作项目的对比等方面，对该项目进行全面深入的解析。

模型协作机制设计

LLM Council 设计了一个三阶段的多模型协作流程，体现了投票表决、同行评议和主席仲裁等机制，保证最终答案经过多方论证与整合：
	•	阶段1：模型独立回答（First opinions） – 用户的问题首先同时发送给委员会中的所有 LLM，每个模型各自独立生成回答 ￼。这一阶段相当于收集“第一印象”式的多样解答，得到一个不同模型答案的集合。前端界面会以标签页形式并列展示这些回答，方便用户逐一对比查看 ￼。在此阶段，各模型彼此之间尚没有互动，每个回答都是在真空中依据自身知识生成的，保证了答案的多样性。
	•	阶段2：模型互评投票（Review） – 接下来，每个模型扮演评审者的角色，对其他模型给出的回答进行匿名互评 ￼。具体做法是将阶段1中所有模型的回答打乱并匿名标注为例如“Response A/B/C…” ￼。然后把原始问题及所有匿名回答组成一个评审提示（prompt），要求模型逐一批判点评每个回答的准确性与见解深度，并给出一个“最终排名 (FINAL RANKING)” ￼ ￼。这个评审 prompt 会并行发送给委员会中的每个模型，让每个模型都对整组答案进行排序投票。由于提示中隐去了哪个回答来自哪个模型，每个 LLM 无法偏袒自身或特定模型，从而保证评审的公平公正 ￼。这一阶段产生的结果包括每个模型给出的完整评语和排名，以及由代码解析提取的名次列表 ￼。项目代码通过正则等手段从模型输出中提取“1. Response X, 2. Response Y…”的排名列表 ￼，随后计算出各回答的平均名次，从而得到一个汇总的群体排名 ￼。可以认为，这一步让所有模型充当彼此的“裁判”，通过群体投票找出了答案中公认最好的部分。
	•	阶段3：主席综合裁决（Final response） – 最后，委员会指定的主席模型（Chairman）登场 ￼。主席模型会拿到阶段1所有模型的回答以及阶段2所有模型的评审和排名结果作为输入，并被要求综合这些信息，产出一个单一的最终答案 ￼。提示会引导主席模型充分考虑各回答的优缺点、模型间的共识与分歧，以及同行评议给出的排名和意见，进而整合形成一个全面且可靠的答复 ￼。主席模型相当于仲裁者或总编辑的角色：它不直接参与第一轮问答和互评投票，而是在最后汇总各方意见给出结论 ￼。这个最终答案代表了模型委员会的“集体智慧”，通常比单一模型回答更完善、有依据 ￼。

经过这三阶段，LLM Council 完成了一次多模型协作问答流程：先独立思考，再相互批评，最后统一定稿。这样的机制充分体现了模型间的分工与合作：多数模型作为委员各抒己见并互评，最终由主席模型仲裁决策，相当于模拟了人类专家小组讨论后形成共识的过程。其中投票和排名机制保证了决策的民主性，主席仲裁机制保证了输出的凝练与一致性。这种设计旨在提高答案的可靠性和准确性，减少单一模型出错或偏颇的风险，也让用户能看到模型之间观点差异与评价，从教学角度了解模型的思考过程 ￼ ￼。

系统架构与实现细节

项目架构概览： LLM Council 的系统采用前后端分离架构，后端由 Python 实现的 FastAPI Web 服务构成，前端是基于 React + Vite 的本地网页应用 ￼ ￼。后端负责 orchestrator（指挥）多个 LLM 完成上述三阶段流程，并提供 REST API 供前端调用；前端则提供一个类似 ChatGPT 界面的聊天 UI，用于显示模型的各阶段结果 ￼。项目的数据持久化采用 JSON 文件存储，每次对话及其消息都保存在data/conversations/目录下，便于重现和离线分析 ￼ ￼。整个系统依赖 OpenRouter 作为统一接口来调用不同厂商的模型 API，这样通过一个 API Key 即可访问 OpenAI、Anthropic、Google 等多个模型，大大简化了并行调用多种模型的实现 ￼ ￼。

核心组件模块： 后端代码组织为几个关键模块，每个模块负责不同的功能，实现清晰的分层解耦 ￼：
	•	config.py – 配置模块。主要用于集中定义和加载系统配置，包括读取 .env 文件中的 OpenRouter API Key，以及设定委员会成员模型列表 (COUNCIL_MODELS) 和主席模型 (CHAIRMAN_MODEL) ￼。通过修改此文件，开发者可以方便地更换或增减参与投票的模型，以及指定哪一个模型担任主席角色 ￼。例如默认配置中列出了 GPT-5.1、Gemini 3 Pro、Claude 4.5、Grok 4 等模型作为委员会成员，并以 Google 的 Gemini 为主席 ￼。这样的设计使得更换模型或切换主席无需修改核心逻辑代码，配置灵活可扩展 ￼。
	•	openrouter.py – LLM 调用客户端。封装了与 OpenRouter 平台交互的HTTP请求逻辑，提供异步函数调用模型接口 ￼。其中关键函数 query_model(model_id, messages) 构造请求头（附上认证 Key）和请求体（指定模型及消息列表），通过 httpx.AsyncClient 调用 OpenRouter 的聊天补全接口，返回模型回复的内容 ￼。为了提高并发性能，openrouter.py 还提供了 query_models_parallel(model_ids, messages)，该函数对给定的多个模型ID启动并发的 query_model 异步任务，并通过 asyncio.gather 一次性等待所有模型都返回结果 ￼。这使得阶段1和阶段2可以同时向所有模型发出请求，而非串行等待逐个模型，极大减少了总延迟 ￼。如果某些模型调用失败或超时，query_models_parallel 会返回 None 表示失败，以便后续逻辑进行过滤和容错处理 ￼。
	•	storage.py – 对话存储模块。提供读写 JSON 文件以记录对话和消息的功能 ￼。每开启一轮新对话，后端会调用 create_conversation(id) 生成一个带有唯一ID、创建时间和默认标题的对话 JSON文件，用于保存该对话过程 ￼。当用户提交问题时，add_user_message 会将用户消息追加到 JSON 的messages列表中 ￼；待三个阶段流程跑完，add_assistant_message 则会把包含阶段1原始回答、阶段2排名结果、阶段3最终答复的综合消息存入该列表 ￼。由于在 assistant 消息中详细保存了每个阶段的数据，开发者能够在会话记录中完整重现委员会内部的推理过程，包括每个模型的回答和评分情况 ￼。此外，list_conversations 可以列出所有历史对话，update_conversation_title 则支持在生成更有意义的标题后修改会话标题 ￼。简而言之，storage.py 构建了一个简易的持久化层，让多轮对话和内部分析结果都被保存下来，以供后续查询和教学分析之用。
	•	council.py – 委员会协作流程核心逻辑。这是整个项目的大脑，实现了之前描述的“三阶段”协作算法 ￼。该模块首先从config.py导入委员会模型列表和主席模型ID，以及从openrouter.py导入刚才介绍的并行查询函数 ￼。接着定义了若干异步函数对应三个阶段的操作，以及辅助的解析和汇总函数。主要的函数包括：
	1.	stage1_collect_responses(user_query: str) – 阶段1：收集模型回答。该函数接收用户查询字符串，将其包装成 OpenAI Chat API 通用格式的消息列表（角色为“user”的一条消息），然后调用前述 query_models_parallel 并行请求所有 COUNCIL_MODELS 列表中的模型 ￼。它会等待所有模型返回各自的回答，将成功的结果整理为{"model": 模型名称, "response": 回答文本}的列表。如果某些模型返回 None（调用失败），则跳过那些模型，只保留成功的回答 ￼。最终返回的 stage1_results 就是各个模型的原始回答列表。这一步实现了模型的无协同独立求解，获取多元答案供后续使用。
	2.	stage2_collect_rankings(user_query: str, stage1_results: List[...]) – 阶段2：匿名互评排序。这个函数负责构造评审提示并收集所有模型的排名反馈 ￼ ￼。实现细节包括：首先将阶段1得到的每个回答赋予一个字母代号（A、B、C、D…），建立一个字典映射label_to_model用于记住字母与模型的对应关系 ￼。然后生成评审 prompt 文本，其中包含：用户原始提问、每个匿名回答的编号和内容，以及对模型的指示——请点评所有回答并给出最终排名 ￼。为了方便解析，提示中特别要求输出一个FINAL RANKING:段落，下列出编号排序，例如“1. Response C / 2. Response A / …”的格式 ￼。构造好提示后，stage2_collect_rankings 再次利用 query_models_parallel 将同一份评审提示并行发送给所有模型（包括主席模型之外的所有委员模型） ￼。每个模型产出的评审结果包含对各个回答的评论和一个最终排名列表。函数会保存每个模型完整的评审文本，同时调用 parse_ranking_from_text 辅助函数从文本中提取出纯粹的排序顺序列表 ￼。例如，如果某模型输出的排名段落是：

FINAL RANKING:  
1. Response C  
2. Response A  
3. Response B  

则 parse_ranking_from_text 会提取出序列 ["Response C", "Response A", "Response B"]，再通过先前的 label_to_model 映射转成具体模型ID的顺序。等所有模型都给出评审后，stage2_collect_rankings 返回每个模型的评审结果列表以及 label_to_model 映射供后用 ￼。接着，council.py 会调用 calculate_aggregate_rankings(stage2_results, label_to_model) 来整合投票结果 ￼。该函数会将每个模型给出的排名列表汇总计算每个回答的平均排名：例如某模型自己的回答在其他模型排名中分别为第2、第1、第3名，则平均名次=2.0 ￼。根据平均名次对模型表现进行排序，就得到整场评审的群体结果（名次越小表示该模型答案越受认可） ￼。阶段2让每个模型都成为裁判，对所有解答进行打分投票，并产出群体智慧评估的排名 ￼。

	3.	stage3_synthesize_final(user_query: str, stage1_results, stage2_results) – 阶段3：主席整合答案。此函数为主席模型准备一个“超级提示（super prompt）”，引导其基于前两个阶段的全部信息生成最终答复 ￼。提示模板一般包含：首先重述用户提问，然后分两个部分提供上下文——阶段1的所有模型回答列表（注明模型ID和回答内容）以及阶段2的所有模型评审及排名结果（注明模型ID和其给出的评审文本，包括最终排名） ￼。最后，在提示末尾对主席模型提出要求：请综合以上所有模型的回答和相互评价，参考它们的共识与差异，给出一个全面且准确的最终答案 ￼。在构建好提示后，stage3_synthesize_final 调用 query_model(CHAIRMAN_MODEL, prompt) 来向主席模型发送请求 ￼。如果主席模型返回失败，则函数输出一个错误结果；如果成功则返回主席模型及其生成的最终答案文本 ￼。这一阶段的实现相当关键——主席模型需要**“阅读”**大量上文（问题、所有回答、所有评论和排名），因此提示构造既要包含充足信息又要避免超过模型上下文长度限制。实际应用中，可能需要选用上下文窗口大的模型担任主席。主席模型输出的结果将作为整个流程的最终答复返回给用户。
此外，council.py 还包含一些辅助功能来完善用户体验和系统健壮性。例如：generate_conversation_title(user_query) 会利用一个快速但廉价的模型（如 Gemini-2.5-flash）来为用户提问生成一个简短的标题，用于对话列表展示 ￼；在对话第一次提问后异步调用生成标题，失败则默认为 “New Conversation” ￼。又如，run_full_council(user_query) 封装了一次提交问答从阶段1到阶段3的完整执行流程，按顺序调用前述三个阶段函数并收集结果，最终返回 (stage1_results, stage2_results, stage3_result, metadata) ￼。其中 metadata 会包含 label_to_model 映射和计算出的 aggregate_rankings 综合排名，方便前端渲染排行榜或供开发者分析 ￼。run_full_council 是最主要的管道，被后端 API 在非流式请求时直接调用来得到全部阶段结果。

	•	main.py – **FastAPI 接口层。**此模块将上述逻辑通过一系列 HTTP 接口暴露出来，并处理前端请求。应用启动时创建 FastAPI app 实例，设置了允许跨域（CORS）以便本地前端 JS 调用 ￼。定义的数据模型包括 Conversation 对象（包含 id、消息列表等）和 ConversationMetadata 等，用于请求和响应格式的约束。主要的接口包括：
	•	GET /：健康检查端点，返回简单状态 JSON（如{"status": "ok"}）证明服务正常运行。
	•	GET /api/conversations：获取所有已有对话的元信息列表，内部调用 storage.list_conversations() 列出对话ID、创建时间、标题和消息数量等 ￼。
	•	POST /api/conversations：创建一个新对话（分配UUID），调用 storage.create_conversation 初始化 JSON 文件，并返回新建的对话对象 ￼。
	•	GET /api/conversations/{id}：获取指定ID的对话完整内容（包括所有消息数组），若不存在则返回404。
上述接口为前端实现多会话管理、查看历史等功能提供了支持。最关键的是以下两个与提问相关的接口：
	•	**POST /api/conversations/{id}/message：非流式的一次性提问接口。**前端将用户输入的问题通过此端点发送，服务端按逻辑依次执行阶段1、2、3完整流程，最后将所有阶段结果一起打包成 JSON 返回 ￼ ￼。其处理步骤为：验证会话存在 -> 将用户问题用storage.add_user_message加入会话 -> （如果是该对话第一条消息则调用generate_conversation_title生成标题并更新存储） ￼ -> 调用前述run_full_council执行所有阶段 -> 用storage.add_assistant_message保存模型的综合回复（其中嵌入了阶段1、2、3的数据） ￼ -> 返回包含 stage1、stage2、stage3结果和元数据的 JSON。 ￼通过这个接口，客户端一次请求即可得到完整的委员会决策过程，非常适合批量调用或需要直接获取最终答案的场景。
	•	POST /api/conversations/{id}/message/stream：流式 SSE 接口。该端点用于支持前端实时逐步显示各阶段结果，采用 Server-Sent Events (SSE) 将阶段结果流式推送 ￼。实现上，main.py 定义了一个异步生成器 event_generator()：它会按照顺序执行阶段1、2、3，在每个阶段开始和结束时通过yield发送事件给前端 ￼。具体流程为：收到用户提问后，先存储用户消息，然后并发启动标题生成任务（不阻塞后续阶段计算） ￼；发送stage1_start事件，等待stage1_collect_responses完成后发送stage1_complete事件附带原始回答结果 ￼；接着stage2_start->执行排名计算->发送stage2_complete事件，内含各模型评审结果及综合排名数据 ￼；然后stage3_start->执行主席模型整合->发送stage3_complete事件及最终答案文本 ￼。最后等待先前的标题任务完成并更新会话标题，发送title_complete事件，保存完整对话消息，结束 SSE 流 ￼。如果中途任一阶段出错，会发送error事件通知前端异常信息 ￼。整个 SSE 通过FastAPI的 StreamingResponse 实现，设置合适的头确保浏览器端稳定接收实时更新 ￼。这种设计使得用户在提交问题后，能够实时看到各模型回答、互评过程和最终答案生成，提升了交互体验和可解释性。

通过以上架构分析可以看出，LLM Council 项目采用了经典的前后端解耦和异步并发技术栈：前端提供直观的多模型对比 UI，后端则高效地并行调用模型并串联逻辑。核心代码充分考虑了可维护性（各模块职责单一、配置集中）和可复现性（对话数据完整保存，方便重跑和分析）。尽管 Karpathy 明确表示这是一个一次性“vibe coding”产物，不会持续维护 ￼，其架构和代码对于开发者却有很高的学习参考价值。例如，如何利用 OpenRouter 这样的聚合服务来调度多模型并行调用，如何设计分阶段的提示串联复杂流程，如何通过JSON日志保存内部决策过程等，这些实现细节都为日后构建多代理协同系统提供了宝贵经验。

Prompt 设计与调用策略

在 LLM Council 项目中，prompt（提示词）的设计与使用策略是整个多模型协作成功的关键。由于需要驱动多个 LLM 在不同阶段扮演不同角色、执行不同任务，项目对提示的构建进行了巧妙的自动化设计，并通过规范输出格式来确保结果可解析。
	•	阶段1提示：直接问答。在第一阶段，每个模型接收到的提示实际上就是用户提问本身。后端将用户问题封装为标准的聊天消息列表格式（通常无额外系统提示或few-shot示例），以“user”身份发送给每个模型 ￼。这里的提示非常简单直接，旨在获得各模型对同一问题的独立回答。由于所有模型并行收到相同的用户提示，这一步没有针对不同模型定制差异化的 prompt，也没有复杂的角色上下文，最大程度利用了模型各自的知识和推理能力。在实现上，prompt 的构建由代码自动完成，开发者只需提供用户输入文本，系统即会生成所需的消息格式并调取模型 ￼。
	•	阶段2提示：匿名评审。第二阶段的 prompt 设计相对复杂，它需要引导模型对多份答案进行批评和排名。项目通过动态拼接字符串的方式自动生成了这个提示内容 ￼。主要包含几个部分：首先是重述一遍原始提问，以提醒模型评审时牢记用户关心的问题 ￼。接下来是按字母编号的匿名回答列表：Prompt 会列出例如“Response A: <答案文本>”、“Response B: <答案文本>”…，将阶段1所有收集到的回答逐一展示，但不透露哪个模型生成了哪个回答 ￼。这种匿名处理非常重要，它相当于给所有答案打乱顺序、去掉来源，以防评审模型因偏见（如对某知名模型偏爱）而不公正评价 ￼。在列出所有待评审回答后，prompt 第三部分是对评审任务的说明和要求：模型被指示要逐条审查每个回答的正确性、全面性等，并写出对每个回答的评语/批判 ￼。最后，也是提示中最关键的一部分，是要求模型给出最终排名（Final Ranking） ￼。为了确保输出易于程序解析，提示明确规定了排名列表的格式：需要以“FINAL RANKING:”开头，后面紧跟编号列表（如“1. Response X”）且不要在排名部分输出无关内容 ￼ ￼。这一严格的格式要求保证了不同模型输出的排名易于用正则等自动化手段提取 ￼。综上，阶段2的 prompt 其实相当长且信息密集：它把问题和所有答案都提供给模型，再施加一个“你现在是评审员”的隐含角色，让模型执行批判和排序的任务。值得注意的是，提示并没有显式告诉模型“你的回答是哪一个”，模型也并不知晓自己在阶段1产出了什么（因为输入不给它看自己的回答）。这样每个模型在评审时，既可能在不知情情况下评价了自己的回答，也可能尖锐地挑出其他回答的问题——恰如真人匿名评审学术论文的过程。这种 prompt 设计使模型彼此既竞争又合作：竞争在于每个模型的答案被其他模型挑剔打分；合作在于它们共同完成了一次集体评议投票 ￼。
	•	阶段3提示：主席综合。第三阶段的提示被称为“超级提示”，因为它包含了前面两个阶段的大量上下文信息 ￼。提示的开头通常会明示角色，例如**“你是一个 LLM 委员会的主席”，以设定模型需要以决策者身份行动 ￼。接下来提示按板块提供信息：原始问题再次呈现，然后是“阶段1 - 各模型的回答”和“阶段2 - 各模型的评审排名” ￼。在阶段1板块中，prompt 会罗列每个模型的名称（或ID）以及其回答内容 ￼；阶段2板块则罗列每个模型对所有回答的评审总结和它给出的排名情况 ￼。通过这种方式，主席模型能够一览整个委员会讨论的过程**：谁说了什么，彼此如何评价，哪些答案更受同行认可。提示的最后，对主席模型提出任务要求，例如：“请综合上述所有模型的回答和它们的互评结果，整合为一个全面且一致的最终答案” ￼。主席模型在这样的提示下，相当于接管了此前人类 moderator 的工作，要客观汇总信息、消除冲突、补充细节，给出更高质量的答案。需要指出，这个提示同样是由代码自动生成的——根据阶段1和2收集的数据拼装而成。因此具备自动化 prompt 构建的特点：开发者无需手工编写这个长提示，系统会在运行时动态插入具体内容（模型名、回答文本、评审文本等）。同时，通过在 prompt 中明确指示模型身份（主席）和任务（综合回答），实现了动态角色指定：即使底层用的可能仍是类似的通用大模型，通过不同的 prompt 内容，它就能在不同时刻切换扮演“答题者”、“评审员”、“主席”等角色。比如在阶段3提示开头显式声明“你是主席” ￼，就有效地将同一模型（如Gemini）置于领导者视角来输出答案，而不是普通助手角度。这种基于 prompt 的角色切换，是当前多代理LLM系统广泛采用的策略，能让模型模拟出不同专业或权限的代理人。

总的来说，LLM Council 的 prompt 设计遵循了逐步增加信息和约束的原则：第一阶段 prompt 最简单，让模型自由回答；第二阶段 prompt 信息量陡增，同时格式要求变严格，让模型认真评判并给结构化输出；第三阶段 prompt 包含前两阶段所有结果，要求模型高层次综合决策。这些 prompt 均由程序自动生成，充分体现了Prompt Engineering在多模型协作中的运用。它不仅涉及对模型的指令编写，还涉及输出格式规范、上下文信息安排和角色定位等多个方面。项目通过精心设计 prompt，使得多个 LLM 能像流水线一样各司其职，又能通过自然语言协议完成协作，最终产出比单一模型更有说服力的结果。这种基于 prompt 的软调度方法，避免了修改模型内部参数或训练专用模型，完全以黑盒方式 orchestrate 了外部现有模型的配合行为，具有很强的实用性和可移植性。

相关项目对比与启发

Karpathy 的 LLM Council 体现了一种将多模型集成 (ensemble)和多代理协同相结合的理念。事实上，近期业界和学术界涌现了许多类似思路的项目和框架，也致力于让多个 LLM 协同工作以完成复杂任务或提高答案质量。下面介绍几个有代表性的项目，并简要比较它们与 LLM Council 在设计理念和技术实现上的异同：
	•	AutoGen – AutoGen 是由微软提出的一个通用多代理对话框架，支持开发者创建多个 LLM代理相互对话来完成任务 ￼ ￼。它强调对话驱动的灵活协作：开发者可以定义不同的 agent 角色，这些代理通过自然语言消息进行交流、角色扮演，并可以动态调整行为来推动任务进展 ￼。例如，一个代理可以负责提出问题，另一个负责解答，还有的负责审核或提供工具调用。AutoGen 的强项在于快速原型和人类中控——开发者无需从零实现通信机制，只需配置代理角色和初始提示，框架就能让它们自动聊天完成任务 ￼。在技术实现上，AutoGen 注重消息历史记忆和调用效率：它提供了对话级的内存来保留多轮交互上下文，并内置了对 LLM 接口调用结果的缓存功能（如用磁盘或 Redis 缓存模型回复）以减少重复开销 ￼。与 LLM Council 相比，AutoGen 并非专门用于问答投票，而是一个更通用的多代理协作平台。LLM Council 的交互是严格三步固定的，而 AutoGen 则支持任意轮数、更加开放式的代理对话流程，适合复杂业务逻辑。二者设计理念上的主要区别在于：LLM Council追求不同模型群体投票提高单次答案准确性，而 AutoGen 更关注将任务拆解给不同专长的代理持续对话求解。从技术上看，LLM Council 的实现较轻量（无长期记忆、无外部工具接口），AutoGen 则提供了丰富的扩展点（如工具集成、长短期记忆模块）供构建复杂系统 ￼ ￼。
	•	ChatDev – ChatDev 是一个多代理协同框架，专门用于模拟软件开发过程的场景 ￼ ￼。它将多个 LLM 设定为虚拟的软件公司成员，各自扮演不同角色（如首席执行官CEO、首席技术官CTO、开发工程师、测试工程师等），协同完成从需求分析、设计、编码到测试的完整软件开发生命周期 ￼ ￼。ChatDev 强调专业化角色分工和阶段式工作流：每个代理在各自擅长的开发阶段发挥作用，例如 CEO 负责总体规划和提出需求，开发工程师根据需求编写代码，测试工程师审查并运行测试用例等 ￼。为确保这些角色代理有效合作，ChatDev 引入了**“ChatChain”工作流和Inception Prompting技术 ￼。Inception Prompting 指为每个角色准备特殊的系统提示，明确其职责、可用工具、沟通协议和停止条件等，使模型始终坚持自己的角色定位进行对话而不串位 ￼ ￼。通过多轮对话，这些代理能够自主完成一个小型软件项目，从无到有产出代码 ￼。相较 LLM Council，ChatDev 的设计更像是在模拟一个多角色团队的长期合作，而不只是一次问答。LLM Council 的模型彼此对等且主要聚焦于同一任务的评判整合**，而 ChatDev 的代理各有分工、上下游衔接，形成流水线式解决复杂任务。技术实现方面，ChatDev 需要处理多轮交互和代码执行，因此可能会结合代码解释器、错误反馈等机制；LLM Council 则没有引入代码执行环节，只关注自然语言输出的比较与整合。ChatDev 给 LLM Council 的启示在于，后者的“主席-委员”结构其实也可以扩展为多角色协作的形式，譬如引入专门的检查者代理审阅答案事实性、引入工具使用代理查询资料等，以完成更复杂的问答或任务。此外，ChatDev 作为研究原型体现的角色扮演+多阶段链式推进思想，对于任何需要多个模型持续互动完成目标的系统都具有参考价值。
	•	CrewAI – CrewAI 是近年来推出的一个多代理协作平台，特点是以职能团队结构来组织代理行为 ￼ ￼。CrewAI 借鉴真实企业团队的分工模式，预设了不同类型的智能体角色（比如数据分析师、策略规划师、执行助手等），每个代理有明确的职责范围和可使用的工具 ￼。开发者可以按照任务需求选择和配置这些代理，让它们协同执行复杂工作流程。CrewAI 强调工作流的结构化，内置支持常见的业务流程模式，并与 LangChain 等生态集成以利用其在检索增强型生成（RAG）和记忆管理方面的能力 ￼ ￼。例如，CrewAI 的每个代理都维护有角色专属的记忆，包含任务上下文、相关实体知识等，以便在长流程中保持连贯 ￼。同时 CrewAI 允许在人类干预点插入检查，确保在关键决策时可控 ￼。从设计理念看，CrewAI 和 ChatDev 有些相似，都是角色明晰的多代理团队，但 CrewAI 更通用，可应用于多种行业场景的自动化流程，而 ChatDev 专注在软件开发域。与 LLM Council 相比，CrewAI 关注的是复杂任务的协同拆解与执行，设计上更多考虑长期多步的计划和并行/分支流程 ￼。LLM Council 则针对单一问答场景提供了一种民主协商式的解题方法。技术上，CrewAI 和 AutoGen 类似采用持续对话和工具使用、记忆模块等架构，而 LLM Council 没有这些附加组件，仅用 prompt 和多模型投票就实现了目标。因此，CrewAI 等框架在功能上比 LLM Council 更复杂更强大，适合构建可拓展的大型代理系统；但反过来 LLM Council 胜在轻量简洁，不依赖庞大的框架，易于理解和复现其思想精华。

总结对比： LLM Council 可以被视为多模型协同的一个特例：它聚焦于同质任务（回答同一个问题）上的模型集成，通过投票和综合来提高结果可信度。而诸如 AutoGen、ChatDev、CrewAI 则探索了更广义的多代理协作——可能涉及异质任务的分解、不同角色的配合以及多轮交互的复杂流程。在 LLM Council 中，各模型（除主席外）身份对等，流程仅一轮问答加一轮评议；在 ChatDev/CrewAI 中，模型有明确层级和职能划分，互动是反复多轮的，最终产出也许不只是一个答案，而可能是完整的代码或决策方案。尽管如此，不同思路之间也有共通之处：比如集思广益和相互审议的思想在 ChatDev 中通过测试代理发现Bug体现了，在 CrewAI 中通过多代理交叉检查体现了；又如角色指定和提示上下文管理在这些系统中都是实现多模型协作的关键手段。Karpathy 的 LLM Council 以不到千行的代码演示了一个强大的想法，即让多个强大的预训练模型彼此配合可以得到更优结果 ￼。这一想法无疑为日后众多多代理框架提供了启发。当然，LLM Council 本身还有很多可扩展之处，例如引入事实查证代理来降低谬误、让模型讨论多轮而非一次评审、或者结合工具查询实时信息等。在多模型协同这个新兴方向上，LLM Council 作为一个易懂的教学范例，展示了投票仲裁式协作的魅力，而 AutoGen、ChatDev、CrewAI 等则展示了分工合作式协作的潜力。展望未来，随着框架和硬件的发展，我们或许可以将这两类思想融合起来，构建出既能博采众长又能各司其职的强大智能体系统，实现1+1>2的效果。

参考文献：
	1.	Karpathy, A. LLM Council works together to answer your hardest questions. GitHub (2025) ￼ ￼.
	2.	Nisarg, N. Andrej Karpathy’s LLM COUNCIL | Fully Explained. Medium (2025) ￼ ￼.
	3.	Nisarg, N. Andrej Karpathy’s LLM COUNCIL | Fully Explained. Medium (2025) ￼ ￼.
	4.	LLM Council – 三阶段架构设计. (Third-party design doc) ￼ ￼.
	5.	IBM. What is ChatDev? (2023) ￼ ￼.
	6.	DataCamp. CrewAI vs LangGraph vs AutoGen: Choosing the Right Multi-Agent AI Framework. (2025) ￼ ￼.