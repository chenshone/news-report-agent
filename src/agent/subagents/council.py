"""ä¸“å®¶å§”å‘˜ä¼š SubAgent

å°è£…å®Œæ•´çš„å››é˜¶æ®µä¸“å®¶åä½œæµç¨‹ï¼š
1. ç‹¬ç«‹åˆ†æ - å„ä¸“å®¶å¹¶è¡Œåˆ†æ
2. äº¤å‰è¯„å®¡ - ä¸“å®¶äº’è¯„
3. å…±è¯†è®¨è®º - å¤„ç†åˆ†æ­§
4. ä¸»ç®¡ç»¼åˆ - æœ€ç»ˆè£å†³
"""

import asyncio
from typing import Any, Dict, List

from deepagents.middleware.subagents import CompiledSubAgent
from langchain_core.messages import AIMessage
from langchain_core.runnables import RunnableConfig, RunnableLambda

from ...config import AppConfig, create_chat_model
from ..council import CROSS_REVIEW_MATRIX, EXPERT_DESCRIPTIONS, generate_cross_review_prompt


# ä¸“å®¶ç³»ç»Ÿæç¤ºè¯ï¼ˆç®€åŒ–ç‰ˆï¼Œé¿å…å¾ªç¯å¯¼å…¥ï¼‰
_EXPERT_PROMPTS = {
    "summarizer": """ä½ æ˜¯æ‘˜è¦ä¸“å®¶ã€‚æå–æ–°é—»çš„æ ¸å¿ƒè¦ç‚¹ï¼Œç”Ÿæˆç»“æ„åŒ–æ‘˜è¦ã€‚
é‡ç‚¹ï¼šè¯†åˆ«å…³é”®äº‹å®ã€ä¸»è¦äººç‰©ã€æ ¸å¿ƒäº‹ä»¶ã€æ—¶é—´çº¿ã€‚
è¾“å‡ºJSONæ ¼å¼ï¼š{key_points: [...], entities: [...], timeline: [...]}""",

    "fact_checker": """ä½ æ˜¯äº‹å®æ ¸æŸ¥ä¸“å®¶ã€‚éªŒè¯å†…å®¹ä¸­çš„å…³é”®å£°æ˜æ˜¯å¦å¯é ã€‚
é‡ç‚¹ï¼šè¯†åˆ«å¯éªŒè¯çš„äº‹å®å£°æ˜ï¼Œè¯„ä¼°æ¥æºå¯é æ€§ï¼Œäº¤å‰éªŒè¯å…³é”®ä¿¡æ¯ã€‚
è¾“å‡ºJSONæ ¼å¼ï¼š{claims: [{claim, verdict, confidence, sources}...]}""",

    "researcher": """ä½ æ˜¯èƒŒæ™¯ç ”ç©¶ä¸“å®¶ã€‚è¡¥å……ç›¸å…³å†å²èƒŒæ™¯å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
é‡ç‚¹ï¼šå…³è”å†å²äº‹ä»¶ï¼Œè¯†åˆ«å…³é”®äººç‰©èƒŒæ™¯ï¼Œæä¾›è¡Œä¸š/é¢†åŸŸä¸Šä¸‹æ–‡ã€‚
è¾“å‡ºJSONæ ¼å¼ï¼š{background: {...}, related_events: [...], context: {...}}""",

    "impact_assessor": """ä½ æ˜¯å½±å“è¯„ä¼°ä¸“å®¶ã€‚è¯„ä¼°çŸ­æœŸå’Œé•¿æœŸå½±å“ï¼Œé¢„æµ‹å‘å±•è¶‹åŠ¿ã€‚
é‡ç‚¹ï¼šåˆ†æç›´æ¥å½±å“ã€é—´æ¥å½±å“ã€æ½œåœ¨é£é™©ã€å‘å±•è¶‹åŠ¿ã€‚
è¾“å‡ºJSONæ ¼å¼ï¼š{short_term: [...], long_term: [...], risks: [...], trends: [...]}""",

    "expert_supervisor": """ä½ æ˜¯ä¸“å®¶å§”å‘˜ä¼šä¸»å¸­ã€‚ç»¼åˆå„ä¸“å®¶åˆ†æï¼Œåšå‡ºæœ€ç»ˆè£å†³ã€‚
èŒè´£ï¼šè¯„ä¼°å„ä¸“å®¶åˆ†æè´¨é‡ï¼Œåè°ƒåˆ†æ­§ï¼Œæ•´åˆæœ€ç»ˆç»“è®ºã€‚""",
}


class ExpertCouncilRunner:
    """
    ä¸“å®¶å§”å‘˜ä¼šæ‰§è¡Œå™¨
    
    å°è£…å››é˜¶æ®µæµç¨‹çš„å®Œæ•´æ‰§è¡Œé€»è¾‘ã€‚
    """
    
    def __init__(self, config: AppConfig):
        """åˆå§‹åŒ–æ‰§è¡Œå™¨"""
        self.config = config
        
        # åˆ›å»ºå„ä¸“å®¶çš„æ¨¡å‹å®ä¾‹
        self.expert_models = {
            "summarizer": create_chat_model(config.model_for_role("summarizer"), config),
            "fact_checker": create_chat_model(config.model_for_role("fact_checker"), config),
            "researcher": create_chat_model(config.model_for_role("researcher"), config),
            "impact_assessor": create_chat_model(config.model_for_role("impact_assessor"), config),
            "expert_supervisor": create_chat_model(config.model_for_role("master"), config),
        }
    
    async def run_council(self, task: str, context: str) -> str:
        """æ‰§è¡Œå®Œæ•´çš„å››é˜¶æ®µä¸“å®¶å§”å‘˜ä¼šæµç¨‹"""
        report_parts = []
        report_parts.append("# ğŸ­ ä¸“å®¶å§”å‘˜ä¼šåˆ†ææŠ¥å‘Š\n")
        report_parts.append(f"**åˆ†æä»»åŠ¡**: {task}\n")
        
        # é˜¶æ®µ 1: ç‹¬ç«‹åˆ†æ
        report_parts.append("\n---\n## é˜¶æ®µ 1: ç‹¬ç«‹åˆ†æ\n")
        expert_outputs = await self._stage1_independent_analysis(task, context)
        
        for expert, output in expert_outputs.items():
            preview = output[:500] + "..." if len(output) > 500 else output
            report_parts.append(f"\n### {expert}\n{preview}\n")
        
        # é˜¶æ®µ 2: äº¤å‰è¯„å®¡
        report_parts.append("\n---\n## é˜¶æ®µ 2: äº¤å‰è¯„å®¡\n")
        reviews, grade_summary = await self._stage2_cross_review(expert_outputs, context)
        
        report_parts.append("\n### è¯„å®¡ç­‰çº§æ±‡æ€»\n")
        for reviewee, grades in grade_summary.items():
            avg_grade = self._calculate_average_grade(grades)
            report_parts.append(f"- **{reviewee}**: {avg_grade} (æ¥è‡ª {len(grades)} ä½è¯„å®¡)\n")
        
        # è¯†åˆ«åˆ†æ­§
        conflicts = self._identify_conflicts(reviews)
        
        # é˜¶æ®µ 3: å…±è¯†è®¨è®º
        discussion_results = ""
        if conflicts:
            report_parts.append(f"\n---\n## é˜¶æ®µ 3: å…±è¯†è®¨è®º\n")
            report_parts.append(f"å‘ç° {len(conflicts)} ä¸ªéœ€è¦è®¨è®ºçš„åˆ†æ­§ç‚¹\n")
            discussion_results = await self._stage3_consensus_discussion(
                conflicts, expert_outputs, context
            )
            report_parts.append(discussion_results)
        else:
            report_parts.append("\n---\n## é˜¶æ®µ 3: å…±è¯†è®¨è®º\n")
            report_parts.append("âœ… ä¸“å®¶æ„è§åŸºæœ¬ä¸€è‡´ï¼Œæ— éœ€é¢å¤–è®¨è®º\n")
        
        # é˜¶æ®µ 4: ä¸»ç®¡ç»¼åˆè£å†³
        report_parts.append("\n---\n## é˜¶æ®µ 4: ä¸»ç®¡ç»¼åˆè£å†³\n")
        final_synthesis = await self._stage4_chairman_synthesis(
            task, expert_outputs, reviews, grade_summary, conflicts, discussion_results
        )
        report_parts.append(final_synthesis)
        
        return "\n".join(report_parts)
    
    async def _stage1_independent_analysis(
        self, task: str, context: str
    ) -> Dict[str, str]:
        """é˜¶æ®µ1: å„ä¸“å®¶ç‹¬ç«‹åˆ†æ"""
        experts = ["summarizer", "fact_checker", "researcher", "impact_assessor"]
        
        async def analyze(expert: str) -> tuple[str, str]:
            model = self.expert_models[expert]
            system_prompt = _EXPERT_PROMPTS[expert]
            
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"è¯·åˆ†æä»¥ä¸‹å†…å®¹:\n\nä»»åŠ¡: {task}\n\nå†…å®¹:\n{context}"}
            ]
            
            try:
                response = await model.ainvoke(messages)
                return expert, response.content
            except Exception as e:
                return expert, f"åˆ†æå¤±è´¥: {str(e)}"
        
        results = await asyncio.gather(*[analyze(expert) for expert in experts])
        return dict(results)
    
    async def _stage2_cross_review(
        self, expert_outputs: Dict[str, str], context: str
    ) -> tuple[List[Dict], Dict[str, List[str]]]:
        """é˜¶æ®µ2: äº¤å‰è¯„å®¡"""
        reviews = []
        grade_summary: Dict[str, List[str]] = {}
        
        async def do_review(reviewer: str, reviewee: str, focus: str) -> Dict:
            model = self.expert_models[reviewer]
            
            prompt = generate_cross_review_prompt(
                reviewer=reviewer,
                reviewee=reviewee,
                reviewee_output=expert_outputs.get(reviewee, ""),
                original_context=context[:1000],
                review_focus=focus,
            )
            
            messages = [
                {"role": "system", "content": EXPERT_DESCRIPTIONS.get(reviewer, "")},
                {"role": "user", "content": prompt}
            ]
            
            try:
                response = await model.ainvoke(messages)
                grade = self._extract_grade(response.content)
                return {
                    "reviewer": reviewer,
                    "reviewee": reviewee,
                    "grade": grade,
                    "content": response.content,
                }
            except Exception as e:
                return {
                    "reviewer": reviewer,
                    "reviewee": reviewee,
                    "grade": "C",
                    "content": f"è¯„å®¡å¤±è´¥: {str(e)}",
                }
        
        # æ”¶é›†æ‰€æœ‰è¯„å®¡ä»»åŠ¡
        review_tasks = []
        for reviewee, reviewer_configs in CROSS_REVIEW_MATRIX.items():
            if reviewee not in expert_outputs:
                continue
            for config in reviewer_configs:
                reviewer = config["reviewer"]
                if reviewer in expert_outputs:
                    review_tasks.append(
                        do_review(reviewer, reviewee, config["focus"])
                    )
        
        # å¹¶è¡Œæ‰§è¡Œ
        results = await asyncio.gather(*review_tasks)
        
        for result in results:
            reviews.append(result)
            reviewee = result["reviewee"]
            if reviewee not in grade_summary:
                grade_summary[reviewee] = []
            grade_summary[reviewee].append(result["grade"])
        
        return reviews, grade_summary
    
    def _identify_conflicts(self, reviews: List[Dict]) -> List[Dict]:
        """è¯†åˆ«éœ€è¦è®¨è®ºçš„åˆ†æ­§ç‚¹"""
        conflicts = []
        for review in reviews:
            grade = review.get("grade", "B")
            if grade in ("C", "D"):
                conflicts.append({
                    "topic": f"{review['reviewer']} å¯¹ {review['reviewee']} çš„è¯„å®¡",
                    "grade": grade,
                    "reviewer": review["reviewer"],
                    "reviewee": review["reviewee"],
                    "content": review.get("content", "")[:300],
                })
        return conflicts
    
    async def _stage3_consensus_discussion(
        self,
        conflicts: List[Dict],
        expert_outputs: Dict[str, str],
        context: str,
    ) -> str:
        """é˜¶æ®µ3: å…±è¯†è®¨è®º"""
        if not conflicts:
            return "æ— éœ€è®¨è®º"
        
        discussions = []
        for conflict in conflicts[:3]:  # æœ€å¤šè®¨è®º 3 ä¸ªåˆ†æ­§
            reviewer = conflict["reviewer"]
            reviewee = conflict["reviewee"]
            
            model = self.expert_models.get(reviewee)
            if not model:
                continue
            
            prompt = f"""ä½ æ˜¯ {reviewee}ï¼Œä½ çš„åˆ†æè¢« {reviewer} è¯„ä¸º {conflict['grade']} çº§ã€‚

{reviewer} çš„è¯„å®¡æ„è§:
{conflict['content']}

è¯·é’ˆå¯¹è¿™äº›æ„è§è¿›è¡Œå›åº”ï¼ˆ200å­—å†…ï¼‰ã€‚
"""
            
            try:
                messages = [{"role": "user", "content": prompt}]
                response = await model.ainvoke(messages)
                discussions.append(f"\n### åˆ†æ­§: {conflict['topic']}\n")
                discussions.append(f"**è¯„å®¡ç­‰çº§**: {conflict['grade']}\n")
                discussions.append(f"**{reviewee} çš„å›åº”**:\n{response.content}\n")
            except Exception as e:
                discussions.append(f"\n### åˆ†æ­§: {conflict['topic']}\n")
                discussions.append(f"è®¨è®ºå¤±è´¥: {str(e)}\n")
        
        return "\n".join(discussions)
    
    async def _stage4_chairman_synthesis(
        self,
        task: str,
        expert_outputs: Dict[str, str],
        reviews: List[Dict],
        grade_summary: Dict[str, List[str]],
        conflicts: List[Dict],
        discussion_results: str,
    ) -> str:
        """é˜¶æ®µ4: ä¸»ç®¡ç»¼åˆè£å†³"""
        model = self.expert_models["expert_supervisor"]
        
        # æ ¼å¼åŒ–è¾“å…¥
        expert_text = "\n\n".join([
            f"### {name}\n{output[:800]}..." if len(output) > 800 else f"### {name}\n{output}"
            for name, output in expert_outputs.items()
        ])
        
        review_text = "\n".join([
            f"- {reviewee}: å¹³å‡ç­‰çº§ {self._calculate_average_grade(grades)}"
            for reviewee, grades in grade_summary.items()
        ])
        
        conflict_text = "\n".join([
            f"- {c['topic']}: ç­‰çº§ {c['grade']}"
            for c in conflicts
        ]) if conflicts else "æ— æ˜æ˜¾åˆ†æ­§"
        
        prompt = f"""ä½ æ˜¯ä¸“å®¶å§”å‘˜ä¼šä¸»å¸­ï¼Œéœ€è¦ç»¼åˆæ‰€æœ‰ä¸“å®¶çš„åˆ†æåšæœ€ç»ˆè£å†³ã€‚

## åŸå§‹ä»»åŠ¡
{task}

## å„ä¸“å®¶ç‹¬ç«‹åˆ†æ
{expert_text}

## äº¤å‰è¯„å®¡ç»“æœ
{review_text}

## åˆ†æ­§ç‚¹
{conflict_text}

## è®¨è®ºç»“æœ
{discussion_results if discussion_results else "ä¸“å®¶æ„è§ä¸€è‡´ï¼Œæœªè¿›è¡Œè®¨è®º"}

---

è¯·ç”Ÿæˆæœ€ç»ˆçš„ç»¼åˆè£å†³æŠ¥å‘Šï¼Œä½¿ç”¨ Markdown æ ¼å¼ã€‚
"""
        
        try:
            messages = [
                {"role": "system", "content": _EXPERT_PROMPTS["expert_supervisor"]},
                {"role": "user", "content": prompt}
            ]
            response = await model.ainvoke(messages)
            return response.content
        except Exception as e:
            return f"ä¸»ç®¡ç»¼åˆå¤±è´¥: {str(e)}"
    
    def _extract_grade(self, text: str) -> str:
        """ä»æ–‡æœ¬ä¸­æå–è¯„å®¡ç­‰çº§"""
        import re
        
        try:
            json_match = re.search(r'"overall_grade"\s*:\s*"([ABCD])"', text, re.IGNORECASE)
            if json_match:
                return json_match.group(1).upper()
        except Exception:
            pass
        
        grade_match = re.search(r'ç­‰çº§[ï¼š:]\s*([ABCD])', text, re.IGNORECASE)
        if grade_match:
            return grade_match.group(1).upper()
        
        return "B"
    
    def _calculate_average_grade(self, grades: List[str]) -> str:
        """è®¡ç®—å¹³å‡ç­‰çº§"""
        if not grades:
            return "B"
        
        grade_values = {"A": 4, "B": 3, "C": 2, "D": 1}
        avg = sum(grade_values.get(g, 3) for g in grades) / len(grades)
        
        if avg >= 3.5:
            return "A"
        elif avg >= 2.5:
            return "B"
        elif avg >= 1.5:
            return "C"
        else:
            return "D"


def create_council(config: AppConfig) -> CompiledSubAgent:
    """
    åˆ›å»ºä¸“å®¶å§”å‘˜ä¼š SubAgent
    
    Args:
        config: åº”ç”¨é…ç½®
        
    Returns:
        é…ç½®å¥½çš„ CompiledSubAgent
    """
    runner = ExpertCouncilRunner(config)
    
    def invoke_fn(state: dict, config_: RunnableConfig | None = None) -> dict:
        """åŒæ­¥è°ƒç”¨"""
        messages = state.get("messages", [])
        if not messages:
            return {"messages": [AIMessage(content="æœªæä¾›åˆ†æä»»åŠ¡")]}
        
        last_message = messages[-1]
        task_content = last_message.content if hasattr(last_message, 'content') else str(last_message)
        
        try:
            result = asyncio.run(runner.run_council(
                task="æ‰§è¡Œä¸“å®¶å§”å‘˜ä¼šåˆ†æ",
                context=task_content,
            ))
        except Exception as e:
            result = f"ä¸“å®¶å§”å‘˜ä¼šæ‰§è¡Œå¤±è´¥: {str(e)}"
        
        return {
            "messages": [
                *messages,
                AIMessage(content=result)
            ]
        }
    
    async def ainvoke_fn(state: dict, config_: RunnableConfig | None = None) -> dict:
        """å¼‚æ­¥è°ƒç”¨"""
        messages = state.get("messages", [])
        if not messages:
            return {"messages": [AIMessage(content="æœªæä¾›åˆ†æä»»åŠ¡")]}
        
        last_message = messages[-1]
        task_content = last_message.content if hasattr(last_message, 'content') else str(last_message)
        
        try:
            result = await runner.run_council(
                task="æ‰§è¡Œä¸“å®¶å§”å‘˜ä¼šåˆ†æ",
                context=task_content,
            )
        except Exception as e:
            result = f"ä¸“å®¶å§”å‘˜ä¼šæ‰§è¡Œå¤±è´¥: {str(e)}"
        
        return {
            "messages": [
                *messages,
                AIMessage(content=result)
            ]
        }
    
    runnable = RunnableLambda(invoke_fn, afunc=ainvoke_fn)
    
    return CompiledSubAgent(
        name="expert_council",
        description="æ‰§è¡Œå®Œæ•´çš„å››é˜¶æ®µä¸“å®¶åä½œæµç¨‹ï¼ˆç‹¬ç«‹åˆ†æâ†’äº¤å‰è¯„å®¡â†’å…±è¯†è®¨è®ºâ†’ä¸»ç®¡ç»¼åˆï¼‰ã€‚è°ƒç”¨æ­¤ agent ä¼šè‡ªåŠ¨åè°ƒæ‰€æœ‰ä¸“å®¶å®Œæˆæ·±åº¦åˆ†æã€‚",
        runnable=runnable,
    )


__all__ = ["create_council", "ExpertCouncilRunner"]

